"""
Virtual Fitting Performance Benchmark
=====================================
ìë™ìœ¼ë¡œ ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê³  ì„±ëŠ¥ì„ ì¸¡ì •í•˜ì—¬ ì‹œê°í™” ë³´ê³ ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

ì¸¡ì • í•­ëª©:
1. Batch Size (1~8)
2. Inference Interval (0.03~0.1)
3. Inference Scale (0.3~1.0)
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # GUI ì—†ì´ ì €ì¥ë§Œ
import time
import os
import sys
from datetime import datetime
import json

# í•œê¸€ í°íŠ¸ ì„¤ì • (Windows)
try:
    plt.rcParams['font.family'] = 'Malgun Gothic'
    plt.rcParams['axes.unicode_minus'] = False
except:
    pass

# fit ë””ë ‰í† ë¦¬ ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
fit_dir = os.path.join(current_dir, 'fit')
if fit_dir not in sys.path:
    sys.path.insert(0, fit_dir)

from fit.virtual_fitting import RTMPoseVirtualFitting

class PerformanceBenchmark:
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir='benchmark_results'):
        """
        Args:
            output_dir: ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.output_dir = os.path.join(current_dir, output_dir)
        os.makedirs(self.output_dir, exist_ok=True)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # í…ŒìŠ¤íŠ¸ ë¹„ë””ì˜¤ (ì›¹ìº  ëŒ€ì‹  ìƒ˜í”Œ í”„ë ˆì„ ìƒì„±)
        self.test_frames = self._generate_test_frames()
        
        print("="*70)
        print("Virtual Fitting Performance Benchmark")
        print("="*70)
        print(f"Output Directory: {self.output_dir}")
        print(f"Timestamp: {self.timestamp}")
        print(f"Test Frames: {len(self.test_frames)}")
        print("="*70 + "\n")
    
    def _generate_test_frames(self, count=100, width=1280, height=720):
        """í…ŒìŠ¤íŠ¸ìš© í”„ë ˆì„ ìƒì„± (ì‹¤ì œ ì¹´ë©”ë¼ ëŒ€ì‹ )"""
        print("[Benchmark] Generating test frames...")
        frames = []
        for i in range(count):
            # ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„±
            frame = np.random.randint(0, 255, (height, width, 3), dtype=np.uint8)
            frames.append(frame)
        print(f"[Benchmark] Generated {count} test frames ({width}x{height})")
        return frames
    
    def benchmark_batch_size(self, batch_sizes=[1, 2, 3, 4, 5, 6, 7, 8]):
        """
        ë°°ì¹˜ í¬ê¸°ì— ë”°ë¥¸ ì„±ëŠ¥ ì¸¡ì •
        
        Args:
            batch_sizes: í…ŒìŠ¤íŠ¸í•  ë°°ì¹˜ í¬ê¸° ë¦¬ìŠ¤íŠ¸
        
        Returns:
            dict: {batch_size: fps}
        """
        print("\n" + "="*70)
        print("Benchmark 1: Batch Size Impact")
        print("="*70)
        
        results = {}
        
        for batch_size in batch_sizes:
            print(f"\n[Test] Batch Size = {batch_size}")
            
            try:
                # VirtualFitting ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
                import torch
                device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
                
                vf = RTMPoseVirtualFitting(
                    cloth_image_path=os.path.join(fit_dir, 'input', 'cloth.jpg'),
                    device=device
                )
                
                # íŒŒë¼ë¯¸í„° ì„¤ì •
                vf.batch_size = batch_size
                vf.use_batch_inference = True
                
                # ì„±ëŠ¥ ì¸¡ì •
                fps = self._measure_fps(vf, num_frames=50)
                results[batch_size] = fps
                
                print(f"[Result] Batch Size {batch_size}: {fps:.2f} FPS")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                vf.stop_inference_thread()
                del vf
                
            except Exception as e:
                print(f"[Error] Batch Size {batch_size} failed: {e}")
                results[batch_size] = 0
        
        return results
    
    def benchmark_inference_interval(self, intervals=None):
        """
        ì¶”ë¡  ê°„ê²©ì— ë”°ë¥¸ ì„±ëŠ¥ ì¸¡ì •
        
        Args:
            intervals: í…ŒìŠ¤íŠ¸í•  ê°„ê²© ë¦¬ìŠ¤íŠ¸ (ì´ˆ)
        
        Returns:
            dict: {interval: fps}
        """
        if intervals is None:
            intervals = [0.03, 0.04, 0.05, 0.067, 0.08, 0.1]
        
        print("\n" + "="*70)
        print("Benchmark 2: Inference Interval Impact")
        print("="*70)
        
        results = {}
        
        for interval in intervals:
            print(f"\n[Test] Inference Interval = {interval:.3f}s ({1/interval:.1f} FPS)")
            
            try:
                import torch
                device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
                
                vf = RTMPoseVirtualFitting(
                    cloth_image_path=os.path.join(fit_dir, 'input', 'cloth.jpg'),
                    device=device
                )
                
                # íŒŒë¼ë¯¸í„° ì„¤ì •
                vf.inference_interval = interval
                vf.use_time_based_inference = True
                
                # ì„±ëŠ¥ ì¸¡ì •
                fps = self._measure_fps(vf, num_frames=50)
                results[interval] = fps
                
                print(f"[Result] Interval {interval:.3f}s: {fps:.2f} FPS")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                vf.stop_inference_thread()
                del vf
                
            except Exception as e:
                print(f"[Error] Interval {interval} failed: {e}")
                results[interval] = 0
        
        return results
    
    def benchmark_inference_scale(self, scales=[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]):
        """
        ì¶”ë¡  í•´ìƒë„ ìŠ¤ì¼€ì¼ì— ë”°ë¥¸ ì„±ëŠ¥ ì¸¡ì •
        
        Args:
            scales: í…ŒìŠ¤íŠ¸í•  ìŠ¤ì¼€ì¼ ë¦¬ìŠ¤íŠ¸
        
        Returns:
            dict: {scale: fps}
        """
        print("\n" + "="*70)
        print("Benchmark 3: Inference Scale Impact")
        print("="*70)
        
        results = {}
        
        for scale in scales:
            print(f"\n[Test] Inference Scale = {scale:.1f} ({int(scale*100)}%)")
            
            try:
                import torch
                device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
                
                vf = RTMPoseVirtualFitting(
                    cloth_image_path=os.path.join(fit_dir, 'input', 'cloth.jpg'),
                    device=device
                )
                
                # íŒŒë¼ë¯¸í„° ì„¤ì •
                vf.inference_scale = scale
                vf.use_inference_downscale = True
                
                # ì„±ëŠ¥ ì¸¡ì •
                fps = self._measure_fps(vf, num_frames=50)
                results[scale] = fps
                
                print(f"[Result] Scale {scale:.1f}: {fps:.2f} FPS")
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                vf.stop_inference_thread()
                del vf
                
            except Exception as e:
                print(f"[Error] Scale {scale} failed: {e}")
                results[scale] = 0
        
        return results
    
    def _measure_fps(self, vf, num_frames=50):
        """
        ì‹¤ì œ FPS ì¸¡ì •
        
        Args:
            vf: VirtualFitting ì¸ìŠ¤í„´ìŠ¤
            num_frames: ì¸¡ì •í•  í”„ë ˆì„ ìˆ˜
        
        Returns:
            float: ì¸¡ì •ëœ FPS
        """
        # ì›Œë°ì—… (ì²˜ìŒ ëª‡ í”„ë ˆì„ì€ ëŠë¦¼)
        for i in range(5):
            frame = self.test_frames[i % len(self.test_frames)]
            _ = vf.process_frame(frame, show_skeleton=False, use_warp=True)
        
        # ì‹¤ì œ ì¸¡ì •
        start_time = time.time()
        
        for i in range(num_frames):
            frame = self.test_frames[i % len(self.test_frames)]
            _ = vf.process_frame(frame, show_skeleton=False, use_warp=True)
        
        elapsed_time = time.time() - start_time
        fps = num_frames / elapsed_time
        
        return fps
    
    def plot_results(self, results_dict, title, xlabel, ylabel, filename):
        """
        ê²°ê³¼ë¥¼ ê·¸ë˜í”„ë¡œ ì‹œê°í™”
        
        Args:
            results_dict: {x: y} í˜•íƒœì˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            title: ê·¸ë˜í”„ ì œëª©
            xlabel: Xì¶• ë¼ë²¨
            ylabel: Yì¶• ë¼ë²¨
            filename: ì €ì¥í•  íŒŒì¼ëª…
        """
        plt.figure(figsize=(12, 7))
        
        x_values = list(results_dict.keys())
        y_values = list(results_dict.values())
        
        # ë§‰ëŒ€ ê·¸ë˜í”„
        bars = plt.bar(range(len(x_values)), y_values, color='steelblue', alpha=0.8, edgecolor='black')
        
        # ê°’ í‘œì‹œ
        for i, (bar, value) in enumerate(zip(bars, y_values)):
            height = bar.get_height()
            plt.text(bar.get_x() + bar.get_width()/2., height,
                    f'{value:.2f}',
                    ha='center', va='bottom', fontsize=10, fontweight='bold')
        
        plt.xlabel(xlabel, fontsize=12, fontweight='bold')
        plt.ylabel(ylabel, fontsize=12, fontweight='bold')
        plt.title(title, fontsize=14, fontweight='bold', pad=20)
        plt.xticks(range(len(x_values)), [str(x) for x in x_values], rotation=45)
        plt.grid(axis='y', alpha=0.3, linestyle='--')
        plt.tight_layout()
        
        # ì €ì¥
        filepath = os.path.join(self.output_dir, f"{self.timestamp}_{filename}")
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"[Saved] {filepath}")
    
    def plot_comparison(self, all_results, filename="comparison_chart.png"):
        """
        ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ê·¸ë˜í”„ë¡œ ë¹„êµ
        
        Args:
            all_results: {'Test Name': {param: fps}} í˜•íƒœ
            filename: ì €ì¥í•  íŒŒì¼ëª…
        """
        plt.figure(figsize=(16, 10))
        
        num_tests = len(all_results)
        
        for i, (test_name, results) in enumerate(all_results.items(), 1):
            plt.subplot(2, 2, i)
            
            x_values = list(results.keys())
            y_values = list(results.values())
            
            bars = plt.bar(range(len(x_values)), y_values, color='coral', alpha=0.7, edgecolor='black')
            
            # ê°’ í‘œì‹œ
            for bar, value in zip(bars, y_values):
                height = bar.get_height()
                plt.text(bar.get_x() + bar.get_width()/2., height,
                        f'{value:.1f}',
                        ha='center', va='bottom', fontsize=9)
            
            plt.title(test_name, fontsize=12, fontweight='bold')
            plt.xlabel('Parameter', fontsize=10)
            plt.ylabel('FPS', fontsize=10)
            plt.xticks(range(len(x_values)), [str(x) for x in x_values], rotation=45)
            plt.grid(axis='y', alpha=0.3)
        
        plt.suptitle('Virtual Fitting Performance Benchmark - All Tests', 
                     fontsize=16, fontweight='bold', y=1.02)
        plt.tight_layout()
        
        # ì €ì¥
        filepath = os.path.join(self.output_dir, f"{self.timestamp}_{filename}")
        plt.savefig(filepath, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"[Saved] {filepath}")
    
    def generate_report(self, all_results):
        """
        í…ìŠ¤íŠ¸ ë³´ê³ ì„œ ìƒì„±
        
        Args:
            all_results: ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        """
        report_path = os.path.join(self.output_dir, f"{self.timestamp}_report.txt")
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*70 + "\n")
            f.write("Virtual Fitting Performance Benchmark Report\n")
            f.write("="*70 + "\n")
            f.write(f"Timestamp: {self.timestamp}\n")
            f.write(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write("="*70 + "\n\n")
            
            for test_name, results in all_results.items():
                f.write(f"\n{test_name}\n")
                f.write("-"*70 + "\n")
                
                for param, fps in results.items():
                    f.write(f"{param:20} : {fps:8.2f} FPS\n")
                
                # ìµœê³  ì„±ëŠ¥
                best_param = max(results, key=results.get)
                best_fps = results[best_param]
                f.write(f"\nBest Performance: {best_param} ({best_fps:.2f} FPS)\n")
                f.write("-"*70 + "\n")
        
        print(f"[Saved] {report_path}")
        
        # JSON í˜•íƒœë¡œë„ ì €ì¥
        json_path = os.path.join(self.output_dir, f"{self.timestamp}_results.json")
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, indent=4)
        
        print(f"[Saved] {json_path}")
    
    def run_all_benchmarks(self):
        """ëª¨ë“  ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        all_results = {}
        
        # 1. Batch Size
        print("\n" + "ğŸ”¥"*35)
        batch_results = self.benchmark_batch_size([1, 2, 3, 4, 5, 6, 7, 8])
        all_results['Batch Size Impact'] = batch_results
        self.plot_results(
            batch_results,
            'Batch Size Impact on FPS',
            'Batch Size',
            'FPS (Frames Per Second)',
            'batch_size_benchmark.png'
        )
        
        # 2. Inference Interval
        print("\n" + "ğŸ”¥"*35)
        interval_results = self.benchmark_inference_interval([0.03, 0.04, 0.05, 0.067, 0.08, 0.1])
        all_results['Inference Interval Impact'] = interval_results
        self.plot_results(
            interval_results,
            'Inference Interval Impact on FPS',
            'Inference Interval (seconds)',
            'FPS (Frames Per Second)',
            'inference_interval_benchmark.png'
        )
        
        # 3. Inference Scale
        print("\n" + "ğŸ”¥"*35)
        scale_results = self.benchmark_inference_scale([0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])
        all_results['Inference Scale Impact'] = scale_results
        self.plot_results(
            scale_results,
            'Inference Scale Impact on FPS',
            'Inference Scale (Resolution %)',
            'FPS (Frames Per Second)',
            'inference_scale_benchmark.png'
        )
        
        # ë¹„êµ ì°¨íŠ¸
        print("\n" + "ğŸ”¥"*35)
        self.plot_comparison(all_results)
        
        # ë³´ê³ ì„œ ìƒì„±
        print("\n" + "ğŸ”¥"*35)
        self.generate_report(all_results)
        
        print("\n" + "="*70)
        print("âœ… All benchmarks completed!")
        print(f"ğŸ“ Results saved to: {self.output_dir}")
        print("="*70)
        
        return all_results


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("\n")
    print("ğŸš€ Starting Virtual Fitting Performance Benchmark...")
    print("\n")
    
    benchmark = PerformanceBenchmark()
    results = benchmark.run_all_benchmarks()
    
    print("\n")
    print("ğŸ‰ Benchmark Complete!")
    print(f"ğŸ“Š Check the results in: {benchmark.output_dir}")
    print("\n")


if __name__ == "__main__":
    main()
