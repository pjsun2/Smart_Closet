"""
ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬ ì†ë„ ë¹„êµ í…ŒìŠ¤íŠ¸
- ìˆœì°¨ ì²˜ë¦¬ vs CUDA Streams ë³‘ë ¬
"""

import sys
import os
import time
import numpy as np
import cv2
import torch

# ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'fit'))

from fit.virtual_fitting import RTMPoseVirtualFitting

def test_sequential_processing(vf, frames):
    """ìˆœì°¨ ì²˜ë¦¬ ì†ë„ ì¸¡ì •"""
    from mmpose.apis import inference_topdown
    
    print("\n" + "="*70)
    print("1ï¸âƒ£ ìˆœì°¨ ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)")
    print("="*70)
    
    start_time = time.time()
    results = []
    
    for frame in frames:
        result = inference_topdown(vf.model, frame)
        results.append(result)
    
    end_time = time.time()
    elapsed = end_time - start_time
    fps = len(frames) / elapsed
    
    print(f"ì²˜ë¦¬ í”„ë ˆì„: {len(frames)}ê°œ")
    print(f"ì´ ì‹œê°„: {elapsed*1000:.2f}ms")
    print(f"í‰ê·  ì‹œê°„: {elapsed*1000/len(frames):.2f}ms/í”„ë ˆì„")
    print(f"FPS: {fps:.2f}")
    
    return elapsed, fps

def test_cuda_streams_parallel(vf, frames):
    """CUDA Streams ë³‘ë ¬ ì²˜ë¦¬ ì†ë„ ì¸¡ì •"""
    from mmpose.apis import inference_topdown
    
    print("\n" + "="*70)
    print("2ï¸âƒ£ CUDA Streams ë³‘ë ¬ (í˜„ì¬ ë°©ì‹)")
    print("="*70)
    
    if not torch.cuda.is_available():
        print("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€, í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ")
        return None, None
    
    start_time = time.time()
    
    # CUDA Streams ìƒì„±
    streams = [torch.cuda.Stream() for _ in range(len(frames))]
    stream_results = [None] * len(frames)
    
    # ê° ìŠ¤íŠ¸ë¦¼ì—ì„œ ë³‘ë ¬ ì¶”ë¡ 
    for i, (frame, stream) in enumerate(zip(frames, streams)):
        with torch.cuda.stream(stream):
            stream_results[i] = inference_topdown(vf.model, frame)
    
    # ëª¨ë“  ìŠ¤íŠ¸ë¦¼ ì™„ë£Œ ëŒ€ê¸°
    torch.cuda.synchronize()
    
    end_time = time.time()
    elapsed = end_time - start_time
    fps = len(frames) / elapsed
    
    print(f"ì²˜ë¦¬ í”„ë ˆì„: {len(frames)}ê°œ")
    print(f"ì´ ì‹œê°„: {elapsed*1000:.2f}ms")
    print(f"í‰ê·  ì‹œê°„: {elapsed*1000/len(frames):.2f}ms/í”„ë ˆì„")
    print(f"FPS: {fps:.2f}")
    
    return elapsed, fps

def test_batch_sizes():
    """ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë¡œ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*70)
    print("ğŸ” ë°°ì¹˜ ë³‘ë ¬ ì²˜ë¦¬ ì†ë„ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print("="*70)
    
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    
    # ê°€ìƒ í”¼íŒ… ì´ˆê¸°í™”
    vf = RTMPoseVirtualFitting(
        cloth_image_path='fit/input/cloth.jpg',
        device=device
    )
    
    # ë”ë¯¸ í”„ë ˆì„ ìƒì„± (640x360)
    print("\nğŸ“ í…ŒìŠ¤íŠ¸ í”„ë ˆì„ ìƒì„±...")
    frames = []
    for i in range(5):
        frame = np.random.randint(0, 255, (360, 640, 3), dtype=np.uint8)
        frames.append(frame)
    print(f"ìƒì„± ì™„ë£Œ: {len(frames)}ê°œ í”„ë ˆì„")
    
    # 1. ìˆœì°¨ ì²˜ë¦¬
    seq_time, seq_fps = test_sequential_processing(vf, frames)
    
    # 2. CUDA Streams ë³‘ë ¬
    parallel_time, parallel_fps = test_cuda_streams_parallel(vf, frames)
    
    # 3. ê²°ê³¼ ë¹„êµ
    print("\n" + "="*70)
    print("ğŸ“Š ê²°ê³¼ ë¹„êµ")
    print("="*70)
    
    if parallel_time and seq_time:
        speedup = seq_time / parallel_time
        print(f"\nìˆœì°¨ ì²˜ë¦¬:")
        print(f"  - ì´ ì‹œê°„: {seq_time*1000:.2f}ms")
        print(f"  - FPS: {seq_fps:.2f}")
        
        print(f"\nCUDA Streams ë³‘ë ¬:")
        print(f"  - ì´ ì‹œê°„: {parallel_time*1000:.2f}ms")
        print(f"  - FPS: {parallel_fps:.2f}")
        
        print(f"\nì†ë„ í–¥ìƒ:")
        print(f"  - {speedup:.2f}ë°° ë¹ ë¦„")
        print(f"  - ì‹œê°„ ì ˆì•½: {(seq_time - parallel_time)*1000:.2f}ms")
        print(f"  - FPS ì¦ê°€: +{parallel_fps - seq_fps:.2f} ({(parallel_fps/seq_fps - 1)*100:.1f}%)")
        
        # GPU ë³‘ë ¬ íš¨ìœ¨ ê³„ì‚°
        ideal_speedup = len(frames)  # ì´ìƒì : 5ë°°
        efficiency = (speedup / ideal_speedup) * 100
        print(f"\nGPU ë³‘ë ¬ íš¨ìœ¨: {efficiency:.1f}% (ì´ìƒì : 100%)")
        
        if efficiency < 50:
            print("âš ï¸ ë‚®ì€ ë³‘ë ¬ íš¨ìœ¨ - mmpose API ì œì•½ ë˜ëŠ” GPU ë³‘ëª©")
        elif efficiency < 80:
            print("âœ… ì–‘í˜¸í•œ ë³‘ë ¬ íš¨ìœ¨")
        else:
            print("ğŸš€ ìš°ìˆ˜í•œ ë³‘ë ¬ íš¨ìœ¨!")
    
    vf.stop_inference_thread()
    
    # ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸
    print("\n" + "="*70)
    print("ğŸ”¬ ë°°ì¹˜ í¬ê¸°ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("="*70)
    
    batch_sizes = [1, 2, 3, 4, 5, 8, 10]
    
    print("\në°°ì¹˜ í¬ê¸° | ìˆœì°¨ ì‹œê°„ | ë³‘ë ¬ ì‹œê°„ | ì†ë„ í–¥ìƒ | íš¨ìœ¨")
    print("-" * 70)
    
    for batch_size in batch_sizes:
        # í”„ë ˆì„ ìƒì„±
        frames = [np.random.randint(0, 255, (360, 640, 3), dtype=np.uint8) 
                  for _ in range(batch_size)]
        
        # ìˆœì°¨ ì²˜ë¦¬
        start = time.time()
        for frame in frames:
            _ = inference_topdown(vf.model, frame)
        seq_time = time.time() - start
        
        # CUDA Streams ë³‘ë ¬
        if torch.cuda.is_available():
            start = time.time()
            streams = [torch.cuda.Stream() for _ in range(len(frames))]
            stream_results = [None] * len(frames)
            
            for i, (frame, stream) in enumerate(zip(frames, streams)):
                with torch.cuda.stream(stream):
                    stream_results[i] = inference_topdown(vf.model, frame)
            
            torch.cuda.synchronize()
            parallel_time = time.time() - start
            
            speedup = seq_time / parallel_time
            efficiency = (speedup / batch_size) * 100
            
            print(f"{batch_size:^10} | {seq_time*1000:>9.1f}ms | {parallel_time*1000:>9.1f}ms | "
                  f"{speedup:>9.2f}x | {efficiency:>6.1f}%")
    
    print("\nğŸ¯ ê²°ë¡ :")
    print("  - ë°°ì¹˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ ë³‘ë ¬ íš¨ìœ¨ ê°ì†Œ (GPU í¬í™”)")
    print("  - ìµœì  ë°°ì¹˜ í¬ê¸°: 3-5ê°œ (íš¨ìœ¨ 70-80%)")
    print("  - mmpose APIëŠ” ë°°ì¹˜ í…ì„œ ë¯¸ì§€ì› â†’ CUDA Streams ìš°íšŒ")

if __name__ == "__main__":
    from mmpose.apis import inference_topdown
    test_batch_sizes()
