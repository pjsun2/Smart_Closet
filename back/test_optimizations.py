"""
ê°€ìƒ í”¼íŒ… ìµœì í™” í…ŒìŠ¤íŠ¸
- ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨
- í ì‹¤ì‹œê°„ì„±
- í”„ë ˆì„ ë³´ê°„
"""

import sys
import os
import time

# ê²½ë¡œ ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'fit'))

from fit.virtual_fitting import RTMPoseVirtualFitting
import torch

def test_initialization():
    """ì´ˆê¸°í™” ë° ì„¤ì • í™•ì¸"""
    print("="*70)
    print("1. ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸")
    print("="*70)
    
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    
    try:
        vf = RTMPoseVirtualFitting(
            cloth_image_path='fit/input/cloth.jpg',
            device=device
        )
        print("\nâœ… ì´ˆê¸°í™” ì„±ê³µ")
        
        # ì„¤ì • í™•ì¸
        print("\nğŸ“Š ìµœì í™” ì„¤ì •:")
        print(f"  - ë°°ì¹˜ í¬ê¸°: {vf.batch_size}")
        print(f"  - ì¶”ë¡  ê°„ê²©: {vf.inference_interval}s ({1/vf.inference_interval:.1f} FPS)")
        print(f"  - ì¶”ë¡  í•´ìƒë„: {int(vf.inference_scale*100)}%")
        print(f"  - ë¹„ë™ê¸° ì¶”ë¡ : {'í™œì„±í™”' if vf.use_async_inference else 'ë¹„í™œì„±í™”'}")
        print(f"  - ë°°ì¹˜ ì²˜ë¦¬: {'í™œì„±í™”' if vf.use_batch_inference else 'ë¹„í™œì„±í™”'}")
        print(f"  - í”„ë ˆì„ ë³´ê°„: {'í™œì„±í™”' if vf.interpolation_enabled else 'ë¹„í™œì„±í™”'}")
        print(f"  - ì‹¤ì œ ì¶”ë¡  ê°„ê²©: {vf.actual_inference_interval}s (ì´ˆê¸°ê°’)")
        
        vf.stop_inference_thread()
        return True
        
    except Exception as e:
        print(f"\nâŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_queue_behavior():
    """í ë™ì‘ í…ŒìŠ¤íŠ¸ (ì˜¤ë˜ëœ í”„ë ˆì„ ì œê±°)"""
    print("\n" + "="*70)
    print("2. í ì‹¤ì‹œê°„ì„± í…ŒìŠ¤íŠ¸ (ì˜¤ë˜ëœ í”„ë ˆì„ ì œê±°)")
    print("="*70)
    
    import queue
    import numpy as np
    import cv2
    
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    
    try:
        vf = RTMPoseVirtualFitting(
            cloth_image_path='fit/input/cloth.jpg',
            device=device
        )
        
        # ë”ë¯¸ í”„ë ˆì„ ìƒì„± (640x360)
        dummy_frame = np.zeros((360, 640, 3), dtype=np.uint8)
        
        print("\nğŸ“ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:")
        print("  1. 10ê°œì˜ í”„ë ˆì„ì„ ë¹ ë¥´ê²Œ ì¶”ê°€")
        print("  2. ì˜¤ë˜ëœ í”„ë ˆì„ì´ ìë™ ì œê±°ë˜ëŠ”ì§€ í™•ì¸")
        print("  3. ìµœì‹  í”„ë ˆì„ë§Œ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸")
        
        # 10ê°œ í”„ë ˆì„ ì¶”ê°€ ì‹œë„
        for i in range(10):
            # í ì´ˆê¸°í™” (ì‹¤ì œ ì½”ë“œ ì‹œë®¬ë ˆì´ì…˜)
            while not vf.inference_queue.empty():
                try:
                    vf.inference_queue.get_nowait()
                except queue.Empty:
                    break
            
            vf.inference_queue.put_nowait((dummy_frame, 1280, 720))
            time.sleep(0.001)  # 1ms
        
        # í í¬ê¸° í™•ì¸
        queue_size = vf.inference_queue.qsize()
        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"  - ì¶”ê°€í•œ í”„ë ˆì„: 10ê°œ")
        print(f"  - íì— ë‚¨ì€ í”„ë ˆì„: {queue_size}ê°œ")
        print(f"  - ì˜¤ë˜ëœ í”„ë ˆì„ ì œê±°: {'ì„±ê³µ' if queue_size == 1 else 'ì‹¤íŒ¨'}")
        
        vf.stop_inference_thread()
        return queue_size == 1
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_batch_efficiency():
    """ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*70)
    print("3. ë°°ì¹˜ ì²˜ë¦¬ íš¨ìœ¨ í…ŒìŠ¤íŠ¸ (ëª¨ë“  ê²°ê³¼ í™œìš©)")
    print("="*70)
    
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    
    try:
        vf = RTMPoseVirtualFitting(
            cloth_image_path='fit/input/cloth.jpg',
            device=device
        )
        
        print("\nğŸ“ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:")
        print("  1. ì›Œì»¤ ìŠ¤ë ˆë“œê°€ ë°°ì¹˜ 5ê°œ ì²˜ë¦¬")
        print("  2. ëª¨ë“  ê²°ê³¼ê°€ result_queueì— ì €ì¥ë˜ëŠ”ì§€ í™•ì¸")
        print("  3. ë°°ì¹˜ íš¨ìœ¨ = (ì‚¬ìš©ëœ ê²°ê³¼ / ì „ì²´ ê²°ê³¼)")
        
        # ì›Œì»¤ ìŠ¤ë ˆë“œê°€ ì‹¤í–‰ ì¤‘ì´ë¯€ë¡œ ê²°ê³¼ í í¬ê¸° í™•ì¸
        time.sleep(2)  # 2ì´ˆ ëŒ€ê¸° (ë°°ì¹˜ ì²˜ë¦¬ ê¸°íšŒ ì œê³µ)
        
        result_count = vf.result_queue.qsize()
        
        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"  - ë°°ì¹˜ í¬ê¸°: {vf.batch_size}")
        print(f"  - ê²°ê³¼ í í¬ê¸°: {result_count}ê°œ")
        print(f"  - ë°°ì¹˜ íš¨ìœ¨: {min(result_count / vf.batch_size * 100, 100):.1f}%")
        print(f"  - ëª¨ë“  ê²°ê³¼ í™œìš©: {'ì„±ê³µ' if result_count > 1 else 'ëŒ€ê¸° ì¤‘'}")
        
        vf.stop_inference_thread()
        return True
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_interpolation():
    """í”„ë ˆì„ ë³´ê°„ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*70)
    print("4. í”„ë ˆì„ ë³´ê°„ í…ŒìŠ¤íŠ¸ (ì‹¤ì œ ê°„ê²© ì‚¬ìš©)")
    print("="*70)
    
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    
    try:
        vf = RTMPoseVirtualFitting(
            cloth_image_path='fit/input/cloth.jpg',
            device=device
        )
        
        print("\nğŸ“ í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:")
        print("  1. ì‹¤ì œ ì¶”ë¡  ê°„ê²© ì¸¡ì •")
        print("  2. ë³´ê°„ ë¹„ìœ¨(alpha) ê³„ì‚°")
        print("  3. ë³´ê°„ì´ ì •ìƒ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸")
        
        # ì›Œì»¤ ìŠ¤ë ˆë“œê°€ ì¶”ë¡  ê°„ê²©ì„ ì¸¡ì •í•  ë•Œê¹Œì§€ ëŒ€ê¸°
        time.sleep(3)
        
        print(f"\nâœ… í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        print(f"  - ì´ë¡ ì  ì¶”ë¡  ê°„ê²©: {vf.inference_interval}s")
        print(f"  - ì‹¤ì œ ì¶”ë¡  ê°„ê²©: {vf.actual_inference_interval:.3f}s")
        print(f"  - ë³´ê°„ í™œì„±í™”: {vf.interpolation_enabled}")
        print(f"  - ì‹¤ì œ ê°„ê²© ë°˜ì˜: {'ì„±ê³µ' if vf.actual_inference_interval > 0 else 'ëŒ€ê¸° ì¤‘'}")
        
        vf.stop_inference_thread()
        return True
        
    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\n" + "="*70)
    print("ğŸ” ê°€ìƒ í”¼íŒ… ìµœì í™” í…ŒìŠ¤íŠ¸")
    print("="*70)
    
    results = {
        'ì´ˆê¸°í™”': test_initialization(),
        'í ì‹¤ì‹œê°„ì„±': test_queue_behavior(),
        'ë°°ì¹˜ íš¨ìœ¨': test_batch_efficiency(),
        'í”„ë ˆì„ ë³´ê°„': test_interpolation(),
    }
    
    print("\n" + "="*70)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼")
    print("="*70)
    
    for name, success in results.items():
        status = "âœ… í†µê³¼" if success else "âŒ ì‹¤íŒ¨"
        print(f"  {name}: {status}")
    
    total = len(results)
    passed = sum(results.values())
    
    print(f"\nì´ {total}ê°œ í…ŒìŠ¤íŠ¸ ì¤‘ {passed}ê°œ í†µê³¼ ({passed/total*100:.1f}%)")
    
    if passed == total:
        print("\nğŸ‰ ëª¨ë“  ìµœì í™”ê°€ ì •ìƒ ì‘ë™í•©ë‹ˆë‹¤!")
    else:
        print("\nâš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

if __name__ == "__main__":
    main()
