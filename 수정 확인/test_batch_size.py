"""
ë°°ì¹˜ í¬ê¸° ìµœì í™” í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
- ë°°ì¹˜ í¬ê¸° 1ë¶€í„° ì‹œì‘í•˜ì—¬ ìµœì ê°’ ìë™ íƒìƒ‰
- ê° ë°°ì¹˜ í¬ê¸°ë‹¹ ì˜ìƒ ì „ì²´ ê¸¸ì´ë§Œí¼ í…ŒìŠ¤íŠ¸
- FPSê°€ 5íšŒ ì—°ì† ê°ì†Œí•˜ë©´ í…ŒìŠ¤íŠ¸ ì¢…ë£Œ
"""

import cv2
import numpy as np
import time
import sys
import os
import threading
import queue
import torch
import matplotlib.pyplot as plt
from matplotlib import font_manager, rc

# í•œê¸€ í°íŠ¸ ì„¤ì •
try:
    # Windows í•œê¸€ í°íŠ¸
    plt.rcParams['font.family'] = 'Malgun Gothic'
    plt.rcParams['axes.unicode_minus'] = False
except:
    pass

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

from mmpose.apis import init_model, inference_topdown

class BatchSizeTester:
    """ë°°ì¹˜ í¬ê¸° ìµœì í™” í…ŒìŠ¤í„°"""
    
    def __init__(self, video_path, device='cuda:0'):
        self.video_path = video_path
        self.device = device
        
        # ë¹„ë””ì˜¤ ê¸¸ì´ í™•ì¸
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise FileNotFoundError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        
        video_fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        video_duration = total_frames / video_fps if video_fps > 0 else 0
        cap.release()
        
        print(f"[í…ŒìŠ¤íŠ¸] ë¹„ë””ì˜¤ ì •ë³´:")
        print(f"  - ì´ í”„ë ˆì„: {total_frames}")
        print(f"  - FPS: {video_fps:.2f}")
        print(f"  - ê¸¸ì´: {video_duration:.2f}ì´ˆ")
        
        # RTMPose ëª¨ë¸ ì´ˆê¸°í™”
        config_file = os.path.join(current_dir, 'models', 'rtmpose-s_8xb256-420e_aic-coco-256x192.py')
        checkpoint_file = os.path.join(current_dir, 'models', 'rtmpose-s_simcc-aic-coco_pt-aic-coco_420e-256x192-fcb2599b_20230126.pth')
        
        print(f"[í…ŒìŠ¤íŠ¸] ëª¨ë¸ ë¡œë”© ì¤‘... (device: {device})")
        self.model = init_model(config_file, checkpoint_file, device=device)
        self.model.eval()
        
        # GPU ì›Œë°ì—…
        if torch.cuda.is_available() and 'cuda' in device:
            print("[í…ŒìŠ¤íŠ¸] GPU ì›Œë°ì—… ì¤‘...")
            torch.backends.cudnn.benchmark = True
            torch.backends.cuda.matmul.allow_tf32 = True
            torch.backends.cudnn.allow_tf32 = True
            
            # ë”ë¯¸ ì´ë¯¸ì§€ë¡œ ì›Œë°ì—… (ì‹¤ì œ ì¶”ë¡  ì‚¬ìš©)
            dummy_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
            with torch.no_grad():
                _ = inference_topdown(self.model, dummy_image)
            torch.cuda.empty_cache()
            print("[í…ŒìŠ¤íŠ¸] GPU ì›Œë°ì—… ì™„ë£Œ")
        
        # í…ŒìŠ¤íŠ¸ ì„¤ì •
        self.queue_size = 22  # ê³ ì • (ì´ì „ í…ŒìŠ¤íŠ¸ ìµœì ê°’)
        self.inference_scale = 0.65  # ê³ ì •
        self.test_duration = video_duration  # ì˜ìƒ ê¸¸ì´ë§Œí¼ í…ŒìŠ¤íŠ¸
        
        print(f"\n[í…ŒìŠ¤íŠ¸] í…ŒìŠ¤íŠ¸ ì„¤ì •:")
        print(f"  - í í¬ê¸°: {self.queue_size} (ê³ ì •)")
        print(f"  - ì¶”ë¡  í•´ìƒë„: {int(self.inference_scale*100)}%")
        print(f"  - í…ŒìŠ¤íŠ¸ ì‹œê°„: {self.test_duration:.2f}ì´ˆ (ì˜ìƒ ì „ì²´ ê¸¸ì´)")
        print(f"  - ë¹„ë””ì˜¤: {video_path}")
    
    def test_single_batch_size(self, batch_size):
        """ë‹¨ì¼ ë°°ì¹˜ í¬ê¸° í…ŒìŠ¤íŠ¸"""
        print(f"\n{'='*70}")
        print(f"[í…ŒìŠ¤íŠ¸] ë°°ì¹˜ í¬ê¸° {batch_size} í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print(f"{'='*70}")
        
        # í ì´ˆê¸°í™”
        inference_queue = queue.Queue(maxsize=self.queue_size)
        result_queue = queue.Queue(maxsize=max(4, self.queue_size // 2))
        running = True
        
        # ì„±ëŠ¥ ì¹´ìš´í„°
        frames_processed = 0
        total_inference_time = 0
        test_start_time = time.time()
        
        # ì¶”ë¡  ì›Œì»¤ ìŠ¤ë ˆë“œ
        def inference_worker():
            nonlocal frames_processed, total_inference_time
            
            while running:
                try:
                    # ë°°ì¹˜ ìˆ˜ì§‘
                    batch_frames = []
                    frame_timeout = 0.015  # 15ms
                    
                    for i in range(batch_size):
                        try:
                            timeout = frame_timeout if i == 0 else frame_timeout * 0.5
                            frame_data = inference_queue.get(timeout=timeout)
                            
                            if frame_data is None:
                                return
                            
                            frame, original_w, original_h = frame_data
                            batch_frames.append(frame)
                        except queue.Empty:
                            break
                    
                    if not batch_frames:
                        continue
                    
                    # ë°°ì¹˜ ì¶”ë¡ 
                    inference_start = time.time()
                    
                    try:
                        results_batch = []
                        if torch.cuda.is_available() and 'cuda' in self.device:
                            streams = [torch.cuda.Stream() for _ in range(len(batch_frames))]
                            stream_results = [None] * len(batch_frames)
                            
                            with torch.no_grad():
                                for i, (frame, stream) in enumerate(zip(batch_frames, streams)):
                                    with torch.cuda.stream(stream):
                                        stream_results[i] = inference_topdown(self.model, frame)
                            
                            torch.cuda.synchronize()
                            results_batch = stream_results
                        else:
                            with torch.no_grad():
                                for frame in batch_frames:
                                    result = inference_topdown(self.model, frame)
                                    results_batch.append(result)
                        
                        inference_end = time.time()
                        inference_time = inference_end - inference_start
                        
                        frames_processed += len(batch_frames)
                        total_inference_time += inference_time
                        
                        # ê²°ê³¼ ì €ì¥ (íê°€ ê°€ë“ ì°¨ë©´ ì˜¤ë˜ëœ ê²ƒ ë²„ë¦¼)
                        try:
                            result_queue.put_nowait(results_batch)
                        except queue.Full:
                            try:
                                result_queue.get_nowait()
                                result_queue.put_nowait(results_batch)
                            except:
                                pass
                    
                    except Exception as e:
                        print(f"[í…ŒìŠ¤íŠ¸] ì¶”ë¡  ì—ëŸ¬: {e}")
                        continue
                
                except Exception as e:
                    if running:
                        print(f"[í…ŒìŠ¤íŠ¸] ì›Œì»¤ ì—ëŸ¬: {e}")
                    continue
        
        # ì›Œì»¤ ìŠ¤ë ˆë“œ ì‹œì‘
        worker_thread = threading.Thread(target=inference_worker, daemon=True)
        worker_thread.start()
        
        # ë¹„ë””ì˜¤ ì½ê¸° ë° í”„ë ˆì„ ì „ì†¡
        cap = cv2.VideoCapture(self.video_path)
        
        if not cap.isOpened():
            print(f"[í…ŒìŠ¤íŠ¸] [ERROR] ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨: {self.video_path}")
            running = False
            return 0, 0
        
        video_fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        print(f"[í…ŒìŠ¤íŠ¸] ë¹„ë””ì˜¤ ì •ë³´: {total_frames} í”„ë ˆì„, {video_fps:.2f} FPS")
        
        frame_count = 0
        frames_sent = 0
        video_start_time = time.time()
        
        while running:
            ret, frame = cap.read()
            
            if not ret:
                # ë¹„ë””ì˜¤ ëë‚˜ë©´ ì¢…ë£Œ
                break
            
            frame_count += 1
            
            # í˜„ì¬ ì‹œê°„ ì²´í¬
            elapsed = time.time() - video_start_time
            if elapsed >= self.test_duration:
                break
            
            # í”„ë ˆì„ ë‹¤ìš´ìŠ¤ì¼€ì¼
            original_h, original_w = frame.shape[:2]
            inference_w = int(original_w * self.inference_scale)
            inference_h = int(original_h * self.inference_scale)
            inference_frame = cv2.resize(frame, (inference_w, inference_h), interpolation=cv2.INTER_LINEAR)
            
            # íì— í”„ë ˆì„ ì¶”ê°€
            try:
                inference_queue.put_nowait((inference_frame, original_w, original_h))
                frames_sent += 1
            except queue.Full:
                pass  # íê°€ ê°€ë“ ì°¨ë©´ í”„ë ˆì„ ë“œë¡­
        
        # ì •ë¦¬
        running = False
        
        # ì¢…ë£Œ ì‹ í˜¸
        try:
            inference_queue.put(None, timeout=0.1)
        except:
            pass
        
        worker_thread.join(timeout=1.0)
        cap.release()
        
        # ê²°ê³¼ ê³„ì‚°
        test_elapsed = time.time() - test_start_time
        actual_fps = frames_processed / test_elapsed if test_elapsed > 0 else 0
        avg_batch_time = total_inference_time / (frames_processed / batch_size) if frames_processed > 0 else 0
        
        # GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        gpu_memory = 0
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_allocated() / 1024**3  # GB
        
        print(f"\n[í…ŒìŠ¤íŠ¸] ë°°ì¹˜ í¬ê¸° {batch_size} ê²°ê³¼:")
        print(f"  - ì²˜ë¦¬ í”„ë ˆì„: {frames_processed} / {frames_sent} (ì „ì†¡ë¨)")
        print(f"  - ì‹¤ì œ FPS: {actual_fps:.2f}")
        print(f"  - í‰ê·  ë°°ì¹˜ ì¶”ë¡  ì‹œê°„: {avg_batch_time*1000:.2f}ms")
        print(f"  - GPU ë©”ëª¨ë¦¬: {gpu_memory:.2f} GB")
        print(f"  - í…ŒìŠ¤íŠ¸ ì‹œê°„: {test_elapsed:.2f}ì´ˆ")
        
        # í ì •ë¦¬
        while not inference_queue.empty():
            try:
                inference_queue.get_nowait()
            except:
                break
        
        while not result_queue.empty():
            try:
                result_queue.get_nowait()
            except:
                break
        
        return actual_fps, gpu_memory
    
    def run_optimization_test(self):
        """ë°°ì¹˜ í¬ê¸° ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"\n{'='*70}")
        print("[í…ŒìŠ¤íŠ¸] ë°°ì¹˜ í¬ê¸° ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("[í…ŒìŠ¤íŠ¸] FPSê°€ 5íšŒ ì—°ì† ê°ì†Œí•˜ë©´ í…ŒìŠ¤íŠ¸ ì¢…ë£Œ")
        print(f"{'='*70}\n")
        
        results = []
        consecutive_decreases = 0
        max_batch_size = 20  # ìµœëŒ€ í…ŒìŠ¤íŠ¸ í¬ê¸°
        
        for batch_size in range(1, max_batch_size + 1):
            fps, gpu_memory = self.test_single_batch_size(batch_size)
            results.append((batch_size, fps, gpu_memory))
            
            # FPS ê°ì†Œ ì²´í¬
            if len(results) >= 2:
                prev_fps = results[-2][1]
                current_fps = results[-1][1]
                
                if current_fps < prev_fps:
                    consecutive_decreases += 1
                    print(f"[í…ŒìŠ¤íŠ¸] FPS ê°ì†Œ ê°ì§€ ({consecutive_decreases}/5): {prev_fps:.2f} â†’ {current_fps:.2f}")
                else:
                    consecutive_decreases = 0
                    print(f"[í…ŒìŠ¤íŠ¸] FPS ì¦ê°€: {prev_fps:.2f} â†’ {current_fps:.2f}")
            
            # 5íšŒ ì—°ì† ê°ì†Œí•˜ë©´ ì¢…ë£Œ
            if consecutive_decreases >= 5:
                print(f"\n[í…ŒìŠ¤íŠ¸] âš ï¸ FPSê°€ 5íšŒ ì—°ì† ê°ì†Œí•˜ì—¬ í…ŒìŠ¤íŠ¸ ì¢…ë£Œ")
                break
            
            # ë‹¤ìŒ í…ŒìŠ¤íŠ¸ ì „ ì ì‹œ ëŒ€ê¸° (GPU ì•ˆì •í™”)
            time.sleep(0.5)
        
        # ê²°ê³¼ ì €ì¥ (ì‹œê°í™”ë¥¼ ìœ„í•´)
        self.test_results = results
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*70}")
        print("[í…ŒìŠ¤íŠ¸] ìµœì¢… ê²°ê³¼")
        print(f"{'='*70}")
        print(f"{'ë°°ì¹˜ í¬ê¸°':<10} | {'FPS':<10} | {'GPU ë©”ëª¨ë¦¬(GB)':<15} | {'ì„±ëŠ¥ ë³€í™”':<15}")
        print(f"{'-'*60}")
        
        max_fps = 0
        optimal_batch_size = 1
        
        for i, (batch_size, fps, gpu_mem) in enumerate(results):
            if i == 0:
                change = "ê¸°ì¤€"
            else:
                prev_fps = results[i-1][1]
                diff = fps - prev_fps
                if prev_fps > 0:
                    change = f"{diff:+.2f} ({diff/prev_fps*100:+.1f}%)"
                else:
                    change = f"{diff:+.2f} (N/A)"
            
            marker = ""
            if fps > max_fps:
                max_fps = fps
                optimal_batch_size = batch_size
                marker = " â­ ìµœê³ "
            
            print(f"{batch_size:<10} | {fps:<10.2f} | {gpu_mem:<15.2f} | {change:<15} {marker}")
        
        print(f"\n{'='*70}")
        print(f"[í…ŒìŠ¤íŠ¸] ğŸ† ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_batch_size} (FPS: {max_fps:.2f})")
        print(f"{'='*70}\n")
        
        # ê¶Œì¥ ì„¤ì • ì¶œë ¥
        print("[í…ŒìŠ¤íŠ¸] ê¶Œì¥ ì„¤ì •:")
        print(f"""
# virtual_fitting.pyì— ì ìš©:
self.batch_size = {optimal_batch_size}
self.inference_queue = queue.Queue(maxsize={self.queue_size})
self.result_queue = queue.Queue(maxsize={max(4, self.queue_size // 2)})
        """)
        
        return optimal_batch_size, max_fps, results
    
    def visualize_results(self, results):
        """ê²°ê³¼ ì‹œê°í™”"""
        print(f"\n[í…ŒìŠ¤íŠ¸] ê²°ê³¼ ì‹œê°í™” ì¤‘...")
        
        batch_sizes = [r[0] for r in results]
        fps_values = [r[1] for r in results]
        gpu_memory = [r[2] for r in results]
        
        # ìµœì ê°’ ì°¾ê¸°
        max_fps = max(fps_values)
        optimal_idx = fps_values.index(max_fps)
        optimal_batch_size = batch_sizes[optimal_idx]
        
        # Figure ìƒì„± (4ê°œ ì„œë¸Œí”Œë¡¯)
        fig, axes = plt.subplots(4, 1, figsize=(14, 16))
        fig.suptitle('ë°°ì¹˜ í¬ê¸° ìµœì í™” í…ŒìŠ¤íŠ¸ ê²°ê³¼', fontsize=16, fontweight='bold')
        
        # 1. FPS vs ë°°ì¹˜ í¬ê¸° (ì„  ê·¸ë˜í”„)
        ax1 = axes[0]
        ax1.plot(batch_sizes, fps_values, 'b-', linewidth=2, marker='o', label='FPS')
        ax1.plot(optimal_batch_size, max_fps, 'r*', markersize=20, label=f'ìµœì ê°’ (ë°°ì¹˜={optimal_batch_size}, FPS={max_fps:.2f})')
        ax1.axhline(y=max_fps, color='r', linestyle='--', alpha=0.3)
        ax1.axvline(x=optimal_batch_size, color='r', linestyle='--', alpha=0.3)
        ax1.set_xlabel('ë°°ì¹˜ í¬ê¸°', fontsize=12)
        ax1.set_ylabel('FPS', fontsize=12)
        ax1.set_title('FPS vs ë°°ì¹˜ í¬ê¸°', fontsize=14, fontweight='bold')
        ax1.grid(True, alpha=0.3)
        ax1.legend(fontsize=10)
        
        # 2. GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (ë§‰ëŒ€ ê·¸ë˜í”„)
        ax2 = axes[1]
        colors_mem = ['orange' if mem < 4 else 'red' if mem > 6 else 'yellow' for mem in gpu_memory]
        ax2.bar(batch_sizes, gpu_memory, color=colors_mem, alpha=0.6)
        ax2.axhline(y=4, color='orange', linestyle='--', alpha=0.5, label='4GB (ê¶Œì¥ ìµœì†Œ)')
        ax2.axhline(y=6, color='red', linestyle='--', alpha=0.5, label='6GB (ê¶Œì¥ ìµœëŒ€)')
        ax2.set_xlabel('ë°°ì¹˜ í¬ê¸°', fontsize=12)
        ax2.set_ylabel('GPU ë©”ëª¨ë¦¬ (GB)', fontsize=12)
        ax2.set_title('ë°°ì¹˜ í¬ê¸°ë³„ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰', fontsize=14, fontweight='bold')
        ax2.grid(True, alpha=0.3, axis='y')
        ax2.legend(fontsize=10)
        
        # 3. ì„±ëŠ¥ ë³€í™”ìœ¨ (ë§‰ëŒ€ ê·¸ë˜í”„)
        ax3 = axes[2]
        changes = [0]  # ì²« ë²ˆì§¸ëŠ” ê¸°ì¤€
        for i in range(1, len(fps_values)):
            if fps_values[i-1] > 0:
                change = (fps_values[i] - fps_values[i-1]) / fps_values[i-1] * 100
            else:
                change = 0  # ì´ì „ FPSê°€ 0ì´ë©´ ë³€í™”ìœ¨ë„ 0
            changes.append(change)
        
        colors = ['green' if c >= 0 else 'red' for c in changes]
        ax3.bar(batch_sizes, changes, color=colors, alpha=0.6)
        ax3.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
        ax3.set_xlabel('ë°°ì¹˜ í¬ê¸°', fontsize=12)
        ax3.set_ylabel('ì„±ëŠ¥ ë³€í™”ìœ¨ (%)', fontsize=12)
        ax3.set_title('ì´ì „ ë°°ì¹˜ í¬ê¸° ëŒ€ë¹„ ì„±ëŠ¥ ë³€í™”ìœ¨', fontsize=14, fontweight='bold')
        ax3.grid(True, alpha=0.3, axis='y')
        
        # 4. ëˆ„ì  ì„±ëŠ¥ ê°œì„  (ì˜ì—­ ê·¸ë˜í”„)
        ax4 = axes[3]
        baseline_fps = fps_values[0]
        cumulative_improvement = [(fps - baseline_fps) / baseline_fps * 100 for fps in fps_values]
        
        ax4.fill_between(batch_sizes, 0, cumulative_improvement, alpha=0.3, color='blue')
        ax4.plot(batch_sizes, cumulative_improvement, 'b-', linewidth=2, marker='o')
        ax4.plot(optimal_batch_size, cumulative_improvement[optimal_idx], 'r*', markersize=20)
        ax4.axhline(y=0, color='black', linestyle='-', linewidth=0.5)
        ax4.axhline(y=cumulative_improvement[optimal_idx], color='r', linestyle='--', alpha=0.3)
        ax4.set_xlabel('ë°°ì¹˜ í¬ê¸°', fontsize=12)
        ax4.set_ylabel('ëˆ„ì  ì„±ëŠ¥ ê°œì„  (%)', fontsize=12)
        ax4.set_title(f'ë°°ì¹˜ í¬ê¸° 1 ëŒ€ë¹„ ëˆ„ì  ì„±ëŠ¥ ê°œì„  (ìµœëŒ€: {cumulative_improvement[optimal_idx]:.1f}%)', 
                     fontsize=14, fontweight='bold')
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ì €ì¥
        output_path = os.path.join(current_dir, 'batch_size_optimization_result.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"[í…ŒìŠ¤íŠ¸] ê·¸ë˜í”„ ì €ì¥: {output_path}")
        
        # í‘œì‹œ
        plt.show()
        
        return output_path


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    video_path = r"C:\Users\parkj\Desktop\ponggwi\source\basic.mp4"
    
    # ë¹„ë””ì˜¤ íŒŒì¼ í™•ì¸
    if not os.path.exists(video_path):
        print(f"[ERROR] ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return
    
    # GPU í™•ì¸
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    print(f"[í…ŒìŠ¤íŠ¸] Device: {device}")
    
    if device == 'cpu':
        print("[WARNING] GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    
    # í…ŒìŠ¤í„° ìƒì„± ë° ì‹¤í–‰
    tester = BatchSizeTester(video_path, device=device)
    optimal_size, max_fps, results = tester.run_optimization_test()
    
    print(f"\n[í…ŒìŠ¤íŠ¸] ì™„ë£Œ! ìµœì  ë°°ì¹˜ í¬ê¸°: {optimal_size}, ìµœëŒ€ FPS: {max_fps:.2f}")
    
    # ì‹œê°í™”
    graph_path = tester.visualize_results(results)
    print(f"\n[í…ŒìŠ¤íŠ¸] ì‹œê°í™” ì™„ë£Œ: {graph_path}")


if __name__ == "__main__":
    main()
