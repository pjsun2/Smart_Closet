"""
ì¶”ë¡  ê°„ê²©(Inference Interval) ìµœì í™” í…ŒìŠ¤íŠ¸
- ì¶”ë¡  ê°„ê²©ì„ 0.02ì´ˆ ~ 0.1ì´ˆê¹Œì§€ ë³€ê²½í•˜ë©° í…ŒìŠ¤íŠ¸
- FPS, GPU ì‚¬ìš©ë¥ , ë ˆì´í„´ì‹œ ì¸¡ì •
- ì‹¤ì œ virtual_fitting.py ì‚¬ìš©
"""

import cv2
import numpy as np
import time
import sys
import os
import subprocess
import matplotlib.pyplot as plt
from matplotlib import font_manager, rc

# í•œê¸€ í°íŠ¸ ì„¤ì •
try:
    plt.rcParams['font.family'] = 'Malgun Gothic'
    plt.rcParams['axes.unicode_minus'] = False
except:
    pass

# í˜„ì¬ ë””ë ‰í† ë¦¬ë¥¼ sys.pathì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

# virtual_fitting import
from virtual_fitting import RTMPoseVirtualFitting
import torch


class InferenceIntervalTester:
    """ì¶”ë¡  ê°„ê²© ìµœì í™” í…ŒìŠ¤í„°"""
    
    def __init__(self, video_path, device='cuda:0'):
        self.video_path = video_path
        self.device = device
        
        # ë¹„ë””ì˜¤ ì •ë³´
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise FileNotFoundError(f"ë¹„ë””ì˜¤ íŒŒì¼ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        
        self.video_fps = cap.get(cv2.CAP_PROP_FPS)
        self.total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        self.video_duration = self.total_frames / self.video_fps if self.video_fps > 0 else 0
        cap.release()
        
        print(f"[í…ŒìŠ¤íŠ¸] ë¹„ë””ì˜¤ ì •ë³´:")
        print(f"  - ì´ í”„ë ˆì„: {self.total_frames}")
        print(f"  - FPS: {self.video_fps:.2f}")
        print(f"  - ê¸¸ì´: {self.video_duration:.2f}ì´ˆ")
    
    def get_gpu_stats(self):
        """GPU ì‚¬ìš©ë¥  ë° ë©”ëª¨ë¦¬ ì¸¡ì •"""
        try:
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,temperature.gpu', 
                 '--format=csv,noheader,nounits'],
                capture_output=True,
                text=True,
                timeout=1
            )
            
            if result.returncode == 0:
                output = result.stdout.strip()
                gpu_util, mem_used, temp = output.split(', ')
                return {
                    'gpu_utilization': float(gpu_util),
                    'memory_used': float(mem_used),
                    'temperature': float(temp)
                }
        except Exception as e:
            pass
        
        return None
    
    def test_single_interval(self, inference_interval):
        """ë‹¨ì¼ ì¶”ë¡  ê°„ê²© í…ŒìŠ¤íŠ¸"""
        print(f"\n{'='*70}")
        print(f"[í…ŒìŠ¤íŠ¸] ì¶”ë¡  ê°„ê²© {inference_interval*1000:.0f}ms ({1/inference_interval:.1f} FPS) í…ŒìŠ¤íŠ¸")
        print(f"{'='*70}")
        
        # Virtual Fitting ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ìƒˆë¡œìš´ ì„¤ì •ìœ¼ë¡œ)
        print("[í…ŒìŠ¤íŠ¸] Virtual Fitting ì´ˆê¸°í™” ì¤‘...")
        try:
            vf = RTMPoseVirtualFitting(
                cloth_image_path='input/cloth.jpg',
                device=self.device
            )
        except Exception as e:
            print(f"[í…ŒìŠ¤íŠ¸] [ERROR] Virtual Fitting ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None
        
        # ì¶”ë¡  ê°„ê²© ë³€ê²½
        vf.inference_interval = inference_interval
        vf.actual_inference_interval = inference_interval
        
        # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
        vf.start_streaming()
        
        print(f"[í…ŒìŠ¤íŠ¸] ì„¤ì • ì™„ë£Œ:")
        print(f"  - ì¶”ë¡  ê°„ê²©: {inference_interval*1000:.0f}ms")
        print(f"  - ë°°ì¹˜ í¬ê¸°: {vf.batch_size}")
        print(f"  - í í¬ê¸°: {vf.inference_queue.maxsize}")
        print(f"  - ì¶”ë¡  í•´ìƒë„: {int(vf.inference_scale*100)}%")
        
        # ì„±ëŠ¥ ì¸¡ì • ë³€ìˆ˜
        frames_processed = 0
        process_times = []
        gpu_stats_samples = []
        
        test_start_time = time.time()
        
        # ë¹„ë””ì˜¤ ì½ê¸°
        cap = cv2.VideoCapture(self.video_path)
        
        if not cap.isOpened():
            print(f"[í…ŒìŠ¤íŠ¸] [ERROR] ë¹„ë””ì˜¤ ì—´ê¸° ì‹¤íŒ¨")
            vf.stop_streaming()
            vf.stop_inference_thread()
            return None
        
        frame_count = 0
        video_start_time = time.time()
        
        # GPU ëª¨ë‹ˆí„°ë§ ì‹œì‘
        last_gpu_check = time.time()
        gpu_check_interval = 0.2  # 200msë§ˆë‹¤ GPU ì²´í¬
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            # ì‹œê°„ ì²´í¬
            elapsed = time.time() - video_start_time
            if elapsed >= self.video_duration:
                break
            
            # í”„ë ˆì„ ì²˜ë¦¬
            process_start = time.time()
            result_frame = vf.process_frame(frame, show_skeleton=False, use_warp=True)
            process_end = time.time()
            
            process_time = process_end - process_start
            process_times.append(process_time)
            frames_processed += 1
            frame_count += 1
            
            # GPU í†µê³„ ìˆ˜ì§‘
            current_time = time.time()
            if current_time - last_gpu_check >= gpu_check_interval:
                stats = self.get_gpu_stats()
                if stats:
                    gpu_stats_samples.append(stats)
                last_gpu_check = current_time
        
        # ì •ë¦¬
        cap.release()
        vf.stop_streaming()
        vf.stop_inference_thread()
        
        # ê²°ê³¼ ê³„ì‚°
        test_elapsed = time.time() - test_start_time
        actual_fps = frames_processed / test_elapsed if test_elapsed > 0 else 0
        
        avg_process_time = np.mean(process_times) * 1000 if process_times else 0
        p50_process_time = np.percentile(process_times, 50) * 1000 if process_times else 0
        p95_process_time = np.percentile(process_times, 95) * 1000 if process_times else 0
        p99_process_time = np.percentile(process_times, 99) * 1000 if process_times else 0
        
        # GPU í†µê³„
        avg_gpu_util = 0
        avg_gpu_mem = 0
        avg_gpu_temp = 0
        
        if gpu_stats_samples:
            avg_gpu_util = np.mean([s['gpu_utilization'] for s in gpu_stats_samples])
            avg_gpu_mem = np.mean([s['memory_used'] for s in gpu_stats_samples])
            avg_gpu_temp = np.mean([s['temperature'] for s in gpu_stats_samples])
        
        print(f"\n[í…ŒìŠ¤íŠ¸] ê²°ê³¼:")
        print(f"  ğŸ“Š ì²˜ë¦¬ëŸ‰:")
        print(f"    - ì²˜ë¦¬ í”„ë ˆì„: {frames_processed}")
        print(f"    - ì‹¤ì œ FPS: {actual_fps:.2f}")
        print(f"    - í…ŒìŠ¤íŠ¸ ì‹œê°„: {test_elapsed:.2f}ì´ˆ")
        
        print(f"\n  â±ï¸ í”„ë ˆì„ ì²˜ë¦¬ ì‹œê°„:")
        print(f"    - í‰ê· : {avg_process_time:.2f}ms")
        print(f"    - P50: {p50_process_time:.2f}ms")
        print(f"    - P95: {p95_process_time:.2f}ms")
        print(f"    - P99: {p99_process_time:.2f}ms")
        
        if gpu_stats_samples:
            print(f"\n  ğŸ® GPU ì‚¬ìš©ë¥ :")
            print(f"    - í‰ê·  ì‚¬ìš©ë¥ : {avg_gpu_util:.1f}%")
            print(f"    - í‰ê·  ë©”ëª¨ë¦¬: {avg_gpu_mem:.0f}MB")
            print(f"    - í‰ê·  ì˜¨ë„: {avg_gpu_temp:.1f}Â°C")
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        # ì•½ê°„ ëŒ€ê¸° (ë‹¤ìŒ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´)
        time.sleep(1.0)
        
        return {
            'inference_interval': inference_interval,
            'target_fps': 1 / inference_interval,
            'frames_processed': frames_processed,
            'actual_fps': actual_fps,
            'avg_process_time': avg_process_time,
            'p50_process_time': p50_process_time,
            'p95_process_time': p95_process_time,
            'p99_process_time': p99_process_time,
            'gpu_util': avg_gpu_util,
            'gpu_mem': avg_gpu_mem,
            'gpu_temp': avg_gpu_temp,
            'process_times': process_times,
            'gpu_stats': gpu_stats_samples
        }
    
    def run_interval_test(self):
        """ì¶”ë¡  ê°„ê²© í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        print(f"\n{'='*70}")
        print("[í…ŒìŠ¤íŠ¸] ì¶”ë¡  ê°„ê²© ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print(f"{'='*70}\n")
        
        # í…ŒìŠ¤íŠ¸í•  ì¶”ë¡  ê°„ê²©ë“¤ (ì´ˆ ë‹¨ìœ„)
        # 0.02ì´ˆ (50 FPS) ~ 0.1ì´ˆ (10 FPS)
        intervals = [
            0.02,   # 50 FPS
            0.025,  # 40 FPS
            0.03,   # 33 FPS
            0.04,   # 25 FPS (í˜„ì¬ ì„¤ì •)
            0.05,   # 20 FPS
            0.067,  # 15 FPS
            0.1,    # 10 FPS
        ]
        
        results = []
        
        for interval in intervals:
            try:
                result = self.test_single_interval(interval)
                if result:
                    results.append(result)
            except Exception as e:
                print(f"[í…ŒìŠ¤íŠ¸] [ERROR] ê°„ê²© {interval*1000:.0f}ms í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        if not results:
            print("[í…ŒìŠ¤íŠ¸] [ERROR] í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # ìµœì¢… ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*70}")
        print("[í…ŒìŠ¤íŠ¸] ì¢…í•© ê²°ê³¼ ë¹„êµ")
        print(f"{'='*70}")
        
        print(f"\n{'ê°„ê²©(ms)':<10} | {'ëª©í‘œFPS':<10} | {'ì‹¤ì œFPS':<10} | {'ì²˜ë¦¬ì‹œê°„(P95)':<15} | {'GPUì‚¬ìš©ë¥ ':<10}")
        print(f"{'-'*70}")
        
        for r in results:
            print(f"{r['inference_interval']*1000:<10.0f} | {r['target_fps']:<10.1f} | "
                  f"{r['actual_fps']:<10.2f} | {r['p95_process_time']:<14.2f}ms | {r['gpu_util']:<9.1f}%")
        
        # ìµœì  ê°„ê²© ì°¾ê¸° (FPSê°€ ê°€ì¥ ë†’ì€ ê²ƒ)
        best_result = max(results, key=lambda x: x['actual_fps'])
        print(f"\n[í…ŒìŠ¤íŠ¸] ğŸ† ìµœì  ì¶”ë¡  ê°„ê²©: {best_result['inference_interval']*1000:.0f}ms "
              f"({best_result['target_fps']:.1f} FPS)")
        print(f"  - ì‹¤ì œ FPS: {best_result['actual_fps']:.2f}")
        print(f"  - GPU ì‚¬ìš©ë¥ : {best_result['gpu_util']:.1f}%")
        
        return results
    
    def visualize_results(self, results):
        """ê²°ê³¼ ì‹œê°í™”"""
        print(f"\n[í…ŒìŠ¤íŠ¸] ê²°ê³¼ ì‹œê°í™” ì¤‘...")
        
        if not results:
            print("[í…ŒìŠ¤íŠ¸] ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        intervals_ms = [r['inference_interval'] * 1000 for r in results]
        target_fps_values = [r['target_fps'] for r in results]
        actual_fps_values = [r['actual_fps'] for r in results]
        p95_times = [r['p95_process_time'] for r in results]
        gpu_utils = [r['gpu_util'] for r in results]
        gpu_mems = [r['gpu_mem'] for r in results]
        
        # Figure ìƒì„± (6ê°œ ì„œë¸Œí”Œë¡¯)
        fig = plt.figure(figsize=(18, 12))
        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)
        
        # 1. FPS ë¹„êµ (ëª©í‘œ vs ì‹¤ì œ)
        ax1 = fig.add_subplot(gs[0, 0])
        x = np.arange(len(intervals_ms))
        width = 0.35
        ax1.bar(x - width/2, target_fps_values, width, label='ëª©í‘œ FPS', color='lightblue', alpha=0.7)
        ax1.bar(x + width/2, actual_fps_values, width, label='ì‹¤ì œ FPS', color='orange', alpha=0.7)
        ax1.set_xlabel('ì¶”ë¡  ê°„ê²© (ms)', fontsize=11)
        ax1.set_ylabel('FPS', fontsize=11)
        ax1.set_title('ëª©í‘œ FPS vs ì‹¤ì œ FPS', fontsize=12, fontweight='bold')
        ax1.set_xticks(x)
        ax1.set_xticklabels([f'{int(i)}' for i in intervals_ms])
        ax1.legend()
        ax1.grid(True, alpha=0.3, axis='y')
        
        # 2. ì‹¤ì œ FPS (ì„  ê·¸ë˜í”„)
        ax2 = fig.add_subplot(gs[0, 1])
        ax2.plot(intervals_ms, actual_fps_values, 'o-', linewidth=2, markersize=8, color='green')
        max_fps_idx = actual_fps_values.index(max(actual_fps_values))
        ax2.plot(intervals_ms[max_fps_idx], actual_fps_values[max_fps_idx], 
                'r*', markersize=20, label=f'ìµœê³  ({actual_fps_values[max_fps_idx]:.2f} FPS)')
        ax2.axhline(y=30, color='blue', linestyle='--', alpha=0.5, label='ëª©í‘œ (30 FPS)')
        ax2.set_xlabel('ì¶”ë¡  ê°„ê²© (ms)', fontsize=11)
        ax2.set_ylabel('ì‹¤ì œ FPS', fontsize=11)
        ax2.set_title('ì¶”ë¡  ê°„ê²©ë³„ ì‹¤ì œ FPS', fontsize=12, fontweight='bold')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. GPU ì‚¬ìš©ë¥ 
        ax3 = fig.add_subplot(gs[0, 2])
        colors_gpu = ['orange' if g < 70 else 'green' if g < 90 else 'red' for g in gpu_utils]
        ax3.bar(intervals_ms, gpu_utils, color=colors_gpu, alpha=0.7)
        ax3.axhline(y=85, color='green', linestyle='--', alpha=0.5, label='ëª©í‘œ (85%)')
        ax3.set_xlabel('ì¶”ë¡  ê°„ê²© (ms)', fontsize=11)
        ax3.set_ylabel('GPU ì‚¬ìš©ë¥  (%)', fontsize=11)
        ax3.set_title('ì¶”ë¡  ê°„ê²©ë³„ GPU ì‚¬ìš©ë¥ ', fontsize=12, fontweight='bold')
        ax3.legend()
        ax3.grid(True, alpha=0.3, axis='y')
        
        # 4. í”„ë ˆì„ ì²˜ë¦¬ ì‹œê°„ (P95)
        ax4 = fig.add_subplot(gs[1, 0])
        ax4.plot(intervals_ms, p95_times, 'o-', linewidth=2, markersize=8, color='purple')
        ax4.set_xlabel('ì¶”ë¡  ê°„ê²© (ms)', fontsize=11)
        ax4.set_ylabel('ì²˜ë¦¬ ì‹œê°„ (ms)', fontsize=11)
        ax4.set_title('í”„ë ˆì„ ì²˜ë¦¬ ì‹œê°„ (P95)', fontsize=12, fontweight='bold')
        ax4.grid(True, alpha=0.3)
        
        # 5. GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        ax5 = fig.add_subplot(gs[1, 1])
        ax5.bar(intervals_ms, gpu_mems, color='purple', alpha=0.7)
        ax5.set_xlabel('ì¶”ë¡  ê°„ê²© (ms)', fontsize=11)
        ax5.set_ylabel('GPU ë©”ëª¨ë¦¬ (MB)', fontsize=11)
        ax5.set_title('ì¶”ë¡  ê°„ê²©ë³„ GPU ë©”ëª¨ë¦¬', fontsize=12, fontweight='bold')
        ax5.grid(True, alpha=0.3, axis='y')
        
        # 6. íš¨ìœ¨ì„± ì ìˆ˜ (FPS / GPU ì‚¬ìš©ë¥ )
        ax6 = fig.add_subplot(gs[1, 2])
        efficiency = [fps / max(gpu, 1) * 100 for fps, gpu in zip(actual_fps_values, gpu_utils)]
        ax6.bar(intervals_ms, efficiency, color='teal', alpha=0.7)
        max_eff_idx = efficiency.index(max(efficiency))
        ax6.bar(intervals_ms[max_eff_idx], efficiency[max_eff_idx], color='gold', alpha=0.9, 
                label=f'ìµœê³  íš¨ìœ¨ ({intervals_ms[max_eff_idx]:.0f}ms)')
        ax6.set_xlabel('ì¶”ë¡  ê°„ê²© (ms)', fontsize=11)
        ax6.set_ylabel('íš¨ìœ¨ì„± ì ìˆ˜', fontsize=11)
        ax6.set_title('íš¨ìœ¨ì„± ì ìˆ˜ (FPS/GPUì‚¬ìš©ë¥  Ã— 100)', fontsize=12, fontweight='bold')
        ax6.legend()
        ax6.grid(True, alpha=0.3, axis='y')
        
        # 7. ì²˜ë¦¬ ì‹œê°„ ë¶„í¬ (ë°•ìŠ¤í”Œë¡¯)
        ax7 = fig.add_subplot(gs[2, 0])
        process_time_data = [r['process_times'] for r in results if r['process_times']]
        if process_time_data:
            process_time_data_ms = [[t * 1000 for t in times] for times in process_time_data]
            bp = ax7.boxplot(process_time_data_ms, labels=[f'{int(i)}' for i in intervals_ms], 
                            patch_artist=True)
            for patch in bp['boxes']:
                patch.set_facecolor('lightgreen')
            ax7.set_xlabel('ì¶”ë¡  ê°„ê²© (ms)', fontsize=11)
            ax7.set_ylabel('ì²˜ë¦¬ ì‹œê°„ (ms)', fontsize=11)
            ax7.set_title('ì²˜ë¦¬ ì‹œê°„ ë¶„í¬', fontsize=12, fontweight='bold')
            ax7.grid(True, alpha=0.3, axis='y')
        
        # 8. FPS ë‹¬ì„±ë¥  (ì‹¤ì œ/ëª©í‘œ)
        ax8 = fig.add_subplot(gs[2, 1])
        achievement_rate = [actual / target * 100 for actual, target in zip(actual_fps_values, target_fps_values)]
        colors_ach = ['green' if a >= 90 else 'orange' if a >= 70 else 'red' for a in achievement_rate]
        ax8.bar(intervals_ms, achievement_rate, color=colors_ach, alpha=0.7)
        ax8.axhline(y=100, color='green', linestyle='--', alpha=0.5, label='100% ë‹¬ì„±')
        ax8.set_xlabel('ì¶”ë¡  ê°„ê²© (ms)', fontsize=11)
        ax8.set_ylabel('ë‹¬ì„±ë¥  (%)', fontsize=11)
        ax8.set_title('FPS ëª©í‘œ ë‹¬ì„±ë¥ ', fontsize=12, fontweight='bold')
        ax8.legend()
        ax8.grid(True, alpha=0.3, axis='y')
        
        # 9. ì¢…í•© ì ìˆ˜ (ë ˆì´ë” ì°¨íŠ¸)
        ax9 = fig.add_subplot(gs[2, 2], projection='polar')
        
        # ì •ê·œí™”ëœ ì ìˆ˜ ê³„ì‚°
        max_fps = max(actual_fps_values)
        max_gpu = max(gpu_utils)
        
        # ê° ê°„ê²©ë³„ ì ìˆ˜
        selected_indices = [0, 3, 6]  # 50 FPS, 25 FPS, 10 FPS
        categories = ['FPS', 'GPUí™œìš©', 'íš¨ìœ¨ì„±', 'ì•ˆì •ì„±']
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]
        
        for idx in selected_indices:
            if idx < len(results):
                r = results[idx]
                fps_score = r['actual_fps'] / max_fps * 100
                gpu_score = r['gpu_util']
                efficiency_score = efficiency[idx] / max(efficiency) * 100
                stability_score = 100 - (r['p95_process_time'] - r['p50_process_time']) / r['p95_process_time'] * 100
                
                values = [fps_score, gpu_score, efficiency_score, stability_score]
                values += values[:1]
                
                label = f"{r['inference_interval']*1000:.0f}ms"
                ax9.plot(angles, values, 'o-', linewidth=2, label=label)
                ax9.fill(angles, values, alpha=0.15)
        
        ax9.set_xticks(angles[:-1])
        ax9.set_xticklabels(categories, fontsize=10)
        ax9.set_ylim(0, 100)
        ax9.set_title('ì¢…í•© ì„±ëŠ¥ ë¹„êµ', fontsize=12, fontweight='bold', pad=20)
        ax9.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
        ax9.grid(True)
        
        plt.suptitle('ì¶”ë¡  ê°„ê²©(Inference Interval) ìµœì í™” í…ŒìŠ¤íŠ¸ ê²°ê³¼', 
                    fontsize=16, fontweight='bold')
        
        # ì €ì¥
        output_path = os.path.join(current_dir, 'inference_interval_optimization_result.png')
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"[í…ŒìŠ¤íŠ¸] ê·¸ë˜í”„ ì €ì¥: {output_path}")
        
        plt.show()
        
        return output_path


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    video_path = r"C:\Users\parkj\Desktop\ponggwi\source\basic.mp4"
    
    if not os.path.exists(video_path):
        print(f"[ERROR] ë¹„ë””ì˜¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return
    
    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
    print(f"[í…ŒìŠ¤íŠ¸] Device: {device}")
    
    if device == 'cpu':
        print("[WARNING] GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    
    # í…ŒìŠ¤í„° ìƒì„± ë° ì‹¤í–‰
    tester = InferenceIntervalTester(video_path, device=device)
    results = tester.run_interval_test()
    
    if results:
        print(f"\n[í…ŒìŠ¤íŠ¸] í…ŒìŠ¤íŠ¸ ì™„ë£Œ! {len(results)}ê°œ ê°„ê²© í…ŒìŠ¤íŠ¸ë¨")
        
        # ì‹œê°í™”
        graph_path = tester.visualize_results(results)
        print(f"\n[í…ŒìŠ¤íŠ¸] ì‹œê°í™” ì™„ë£Œ: {graph_path}")


if __name__ == "__main__":
    main()
