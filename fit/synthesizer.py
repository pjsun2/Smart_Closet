# íŒŒì¼ëª…: synthesizer.py

from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler
import torch
from PIL import Image

def synthesize_final_image(pose_image, warped_cloth_image, output_filename):
    """ë””í“¨ì „ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ì´ë¯¸ì§€ë¥¼ í•©ì„±í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•œë‹¤."""
    print("ğŸš€ 4ë‹¨ê³„ ì‹œì‘: ìµœì¢… ì´ë¯¸ì§€ í•©ì„± (GPU ì‚¬ìš©)")

    # ê²½ëŸ‰í™”ëœ ëª¨ë¸ ë˜ëŠ” í‘œì¤€ ëª¨ë¸ ë¡œë“œ
    base_model_path = "runwayml/stable-diffusion-v1-5"
    controlnet_path = "lllyasviel/sd-controlnet-openpose"

    controlnet = ControlNetModel.from_pretrained(controlnet_path, torch_dtype=torch.float16)
    pipe = StableDiffusionControlNetPipeline.from_pretrained(
        base_model_path,
        controlnet=controlnet,
        torch_dtype=torch.float16,
        safety_checker=None # ì•ˆì „ ê²€ì‚¬ê¸° ë¹„í™œì„±í™” (ì„ íƒ ì‚¬í•­)
    ).to("cuda")

    # ì†ë„ í–¥ìƒì„ ìœ„í•œ Scheduler ì„¤ì •
    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)

    # (ì„ íƒ) xformersë¥¼ í†µí•œ ë©”ëª¨ë¦¬ ìµœì í™”
    # pipe.enable_xformers_memory_efficient_attention()

    # PIL Imageë¡œ ë³€í™˜
    pose_pil = Image.fromarray(pose_image)
    
    prompt = "a person is wearing a new stylish t-shirt, photorealistic, high quality, full body shot"
    negative_prompt = "bad anatomy, ugly, deformed, malformed, worst quality, low quality"

    # ì´ë¯¸ì§€ ìƒì„±
    generator = torch.Generator(device="cuda").manual_seed(42) # ê²°ê³¼ ì¬í˜„ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
    result_image = pipe(
        prompt,
        negative_prompt=negative_prompt,
        image=pose_pil,
        num_inference_steps=20,
        generator=generator
    ).images[0]
    
    result_image.save(output_filename)
    print(f"âœ… 4ë‹¨ê³„ ì™„ë£Œ: ìµœì¢… ì´ë¯¸ì§€ê°€ '{output_filename}'ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")